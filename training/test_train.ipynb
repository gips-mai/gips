{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, df) -> None:\n",
    "        self.dataframe = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        series = self.dataframe.iloc[index]\n",
    "        # Convert the pandas Series to a numpy array\n",
    "        array = series.values\n",
    "        # Convert the numpy array to a tensor\n",
    "        tensor = torch.from_numpy(array)\n",
    "        return tensor\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        descript = torch.tensor(row['descriptions'])\n",
    "        img_embed = torch.tensor(row['img_emb'])\n",
    "        country_encoding = torch.tensor(row['country_enc'])\n",
    "        cell_target = torch.tensor(row['cell_target'])\n",
    "        coordinate_target = torch.tensor(row['coordinate_target'])\n",
    "\n",
    "        # return {\n",
    "        #     'descript': descript,\n",
    "        #     'img_embed': img_embed,\n",
    "        #     'country_encoding': country_encoding,\n",
    "        #     'cell_target': cell_target,\n",
    "        #     'coordinate_target': coordinate_target\n",
    "        # }\n",
    "\n",
    "        return descript, img_embed, country_encoding, cell_target, coordinate_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description Emb., Img Emb., Country Hot Encoding, Cell Target, Coordinate Target (Lat, Lon)\n",
    "n = 124\n",
    "descripts = [torch.randn(716).numpy() for i in range(n)]\n",
    "img_embeds = [torch.randn(716).numpy() for i in range(n)]\n",
    "\n",
    "country_encodings = []\n",
    "for i in range(n):\n",
    "    enc = torch.zeros(221).numpy()\n",
    "    enc[torch.randint(0, 221, (1, ))[0]] = 1\n",
    "    country_encodings.append(enc)\n",
    "\n",
    "cell_targets = [torch.randint(0, 10000, (1, ))[0].numpy() for i in range(n)]\n",
    "\n",
    "coordinate_targets = []\n",
    "for i in range(n):\n",
    "    enc = torch.randn(2).numpy()\n",
    "    enc[0] *= 180\n",
    "    enc[1] *= 90\n",
    "    coordinate_targets.append(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(list(zip(descripts, img_embeds, country_encodings, cell_targets, coordinate_targets)),\n",
    "               columns =['descriptions', 'img_emb', \"country_enc\", \"cell_target\", \"coordinate_target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd_dataset = CustomDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for batch in torch.utils.data.DataLoader(pd_dataset):\n",
    "    # print(batch)\n",
    "    print(type(batch[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/710gz2k50d1gq3j45sxngl8w0000gn/T/ipykernel_9842/644826288.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  clue_embeddings = torch.tensor(list(clue_embeddings.values()))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from model.attention_module import AttentionWeightedAggregation, LinearAttention\n",
    "# from model.backbone import LatLongHead, StreetCLIP, TextEncoder #TODO remove unused imports\n",
    "from model.country_prediction import CountryClassifier\n",
    "from model.head.geolocation_head import MLPCentroid, HybridHeadCentroid\n",
    "\n",
    "from model.attention_module import get_pseudo_label_loss\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "clue_embeddings = pd.read_pickle('../data/guidebook_roberta_base_ch_in.pkl')\n",
    "clue_embeddings = torch.tensor(list(clue_embeddings.values()))\n",
    "clues = load_dataset(\"gips-mai/all_clues_enc\")['train'][:len(clue_embeddings)]\n",
    "clues['encoding'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768])\n",
      "torch.Size([768, 3817])\n",
      "divisor 2931456\n",
      "agg torch.Size([16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x16 and 1484x1024)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 57\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124magg\u001B[39m\u001B[38;5;124m'\u001B[39m, weighted_aggregation\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# country loss\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mcountry_classifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweighted_aggregation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m country_loss \u001B[38;5;241m=\u001B[39m country_classifier\u001B[38;5;241m.\u001B[39mtraining_step(x\u001B[38;5;241m=\u001B[39mweighted_aggregation, target\u001B[38;5;241m=\u001B[39mcountry_target) \u001B[38;5;66;03m# target: get the iso2 of actual country and then look at the one hot encoding\u001B[39;00m\n\u001B[1;32m     59\u001B[0m country_losses\u001B[38;5;241m.\u001B[39mappend(country_loss)\n",
      "File \u001B[0;32m~/Developer/gips/model/country_prediction.py:38\u001B[0m, in \u001B[0;36mCountryClassifier.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msoftmax(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.10/envs/gips/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.10/envs/gips/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.10/envs/gips/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.10/envs/gips/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.10/envs/gips/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.10/envs/gips/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x16 and 1484x1024)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### HYPER PARAMETERS ###\n",
    "lr = 0.001\n",
    "alpha = 0.75\n",
    "use_tanh = True\n",
    "scale_tanh = 1.2\n",
    "### HYPER PARAMETERS ###\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "clue_embedding_size:int = 768\n",
    "text_embedding_size:int = 512\n",
    "clip_embedding_size:int = 716\n",
    "\n",
    "country_encoding = pd.read_csv('../data/encodings.csv')\n",
    "\n",
    "attention_aggregation = AttentionWeightedAggregation(temperature=0.01) #TODO definde temperature\n",
    "linear_attention = LinearAttention(attn_input_img_size=clip_embedding_size, text_features_size=clue_embedding_size, hidden_layer_size_0=1024, hidden_layer_size_1=1024) #TODO hidden layer size\n",
    "country_classifier = CountryClassifier(clue_embedding_size=clue_embedding_size, image_embedding_size=clip_embedding_size, alpha=alpha)\n",
    "\n",
    "previous_stage_output = text_embedding_size+clip_embedding_size+clue_embedding_size\n",
    "geohead = MLPCentroid(initial_dim=previous_stage_output, hidden_dim=[previous_stage_output, 1024, 512])\n",
    "hybrid_head_centroid = HybridHeadCentroid(final_dim=11398, quadtree_path='../data/quad_tree/quadtree_10_1000.csv', use_tanh=use_tanh, scale_tanh=scale_tanh)\n",
    "\n",
    "optimizer = optim.Adam(list(country_classifier.parameters()) + list(geohead.parameters()))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5)\n",
    "\n",
    "cell_loss = nn.CrossEntropyLoss()\n",
    "coordinate_loss = nn.MSELoss()\n",
    "\n",
    "# clues = load_dataset(\"gips-mai/all_clues_enc\")['train'][:20]\n",
    "# clue_embeddings = torch.tensor(clues['encoding'])\n",
    "\n",
    "\n",
    "# clues = load_dataset(\"gips-mai/all_clues_enc\")['train'][:2]\n",
    "\n",
    "pseudo_label_loss = get_pseudo_label_loss(clues[\"country_one_hot_enc\"])\n",
    "# descriptions = load_dataset(\"gips-mai/enc_descr\")\n",
    "# data_loader = torch.utils.data.DataLoader(descriptions, batch_size=32, shuffle=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(pd_dataset, batch_size = 16, shuffle=False)\n",
    "\n",
    "country_losses = []\n",
    "geo_losses = []\n",
    "for epoch in range(10):\n",
    "    for batch in data_loader:\n",
    "        descriptions, imgs, country_target, cell_target, coordinate_target = batch\n",
    "        imgs, descriptions, country_target, cell_target, coordinate_target = imgs.to(device), descriptions.to(device), country_target.to(device), cell_target.to(device), coordinate_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        attention = linear_attention.forward(img_embedding=imgs)\n",
    "        weighted_aggregation = attention_aggregation.forward(clue_embeddings=clue_embeddings, attention=attention)\n",
    "\n",
    "        country_loss = country_classifier.training_step(x=weighted_aggregation, target=country_target) # target: get the iso2 of actual country and then look at the one hot encoding\n",
    "        country_losses.append(country_loss)\n",
    "\n",
    "        # pseudo label loss\n",
    "        current_pseudo_label_loss = pseudo_label_loss(country_target, attention)\n",
    "\n",
    "        aux_attention_loss = alpha * current_pseudo_label_loss + (1-alpha) * country_loss\n",
    "\n",
    "        aggregated_input = torch.cat([imgs, descriptions, weighted_aggregation], dim=1)\n",
    "\n",
    "        intermediate = geohead.forward(aggregated_input)\n",
    "        prediction = hybrid_head_centroid.forward(intermediate, cell_target)\n",
    "\n",
    "        total_loss = cell_loss.apply(prediction['label'], cell_target) + \\\n",
    "                     coordinate_loss.apply(prediction['gps'], coordinate_target) + \\\n",
    "                     aux_attention_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the loss at each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch+1}, Loss: {country_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mx\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(country_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[0,1], [0,1]])\n",
    "y[0] = torch.tensor([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/710gz2k50d1gq3j45sxngl8w0000gn/T/ipykernel_8340/3223471952.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  torch.tensor(list(clue_embeddings.values()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0618,  0.1175, -0.0703,  ..., -0.0676,  0.0133, -0.0567],\n",
       "        [-0.0505,  0.1200, -0.0689,  ..., -0.0360,  0.0539, -0.0521],\n",
       "        [-0.0460,  0.1177, -0.0573,  ..., -0.0460,  0.0242, -0.0192],\n",
       "        ...,\n",
       "        [-0.0966,  0.1018, -0.0211,  ..., -0.0497, -0.0341, -0.0596],\n",
       "        [-0.0636,  0.1438, -0.0114,  ..., -0.0722, -0.0099, -0.0941],\n",
       "        [-0.0786,  0.1549,  0.0098,  ..., -0.0329,  0.0091, -0.0597]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(list(clue_embeddings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
